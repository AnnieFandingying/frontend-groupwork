"""
çˆ¬å–åˆå§‹æ–°é—»æ•°æ®è„šæœ¬
è¿è¡Œæ­¤è„šæœ¬ç”Ÿæˆ initial_news_data.json æ–‡ä»¶
"""
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from app.services.news_crawler import news_crawler
from app.core.database import SessionLocal
import json

def main():
    print("ğŸš€ å¼€å§‹çˆ¬å–åˆå§‹æ–°é—»æ•°æ®...")
    print("=" * 60)
    
    db = SessionLocal()
    try:
        # æ‰§è¡Œå®Œæ•´çˆ¬å–
        result = news_crawler.crawl_all(db)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆ:")
        print(f"   æ€»è®¡çˆ¬å–: {result['total_crawled']} æ¡")
        print(f"   ä¿å­˜æ–°æ•°æ®: {result['saved_count']} æ¡")
        print(f"\nå„æºç»Ÿè®¡:")
        for source, count in result['sources'].items():
            print(f"   - {source}: {count} æ¡")
        
        # ä»æ•°æ®åº“è¯»å–æ‰€æœ‰æ–°é—»
        from app.models.models import NewsItem
        all_news = db.query(NewsItem).order_by(NewsItem.published_at.desc()).all()
        
        # è½¬æ¢ä¸ºJSONæ ¼å¼
        news_data = []
        for item in all_news:
            news_data.append({
                'title': item.title,
                'url': item.url,
                'source': item.source,
                'summary': item.summary or '',
                'tags': json.loads(item.tags) if item.tags else [],
                'published_at': item.published_at.isoformat() if item.published_at else None
            })
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = os.path.join(os.path.dirname(__file__), 'data', 'initial_news_data.json')
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆå§‹æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   å…± {len(news_data)} æ¡æ–°é—»")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()

if __name__ == "__main__":
    main()
