from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Dict
import json

from ....core.database import get_db
from ....models.models import ChatHistory
from ....schemas.chat import StreamChatRequest, ChatHistoryResponse, SuccessResponse
from ....services.ai_service import ai_service
from ....services.rag_service import rag_service
from pydantic import BaseModel

router = APIRouter()


class CodeAssistRequest(BaseModel):
    """ä»£ç è¾…åŠ©è¯·æ±‚"""
    code: str
    action: str  # "explain", "complete", "fix"
    language: str = "javascript"
    context: str = ""  # é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯


@router.post("/stream")
async def chat_stream_endpoint(request: StreamChatRequest):
    """
    æµå¼èŠå¤©æ¥å£ - ä¸å‰ç«¯å¯¹æ¥
    """
    user_message = request.message
    
    # Search for relevant knowledge nodes using RAG
    related_nodes = []
    context = ""
    
    if request.use_rag:
        try:
            relevant = rag_service.search_relevant_nodes(user_message, top_k=3)
            related_nodes = [node['node_id'] for node in relevant]
            context = rag_service.build_context_from_nodes(relevant)
            print(f"Found {len(relevant)} relevant knowledge nodes")
        except Exception as e:
            print(f"RAG search error: {e}")
    
    # Convert history to proper format
    formatted_history = []
    for msg in request.history:
        if isinstance(msg, dict) and "role" in msg and "content" in msg:
            formatted_history.append({
                "role": "user" if msg["role"] == "user" else "assistant",
                "content": msg["content"]
            })
    
    async def generate():
        try:
            # å‘é€å¼€å§‹æ ‡è®°
            yield f"data: {json.dumps({'chunk': '', 'related_nodes': related_nodes})}\n\n"
            
            # æµå¼è·å–AIå›å¤
            async for chunk in ai_service.chat_stream_with_context(
                user_message,
                context,
                formatted_history
            ):
                # ä»¥ SSE æ ¼å¼è¿”å›
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            
            # å‘é€ç»“æŸæ ‡è®°
            yield f"data: {json.dumps({'chunk': '', 'finished': True})}\n\n"
            
        except Exception as e:
            error_msg = f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"
            yield f"data: {json.dumps({'chunk': error_msg, 'error': True})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )


@router.post("/message")
async def send_message(
    request: StreamChatRequest,
    user_id: int = 1,  # æš‚æ—¶ç¡¬ç¼–ç ï¼Œåç»­å¯ä»¥ä»è®¤è¯ä¸­è·å–
    db: Session = Depends(get_db)
):
    """Send a chat message and get AI response (éæµå¼)"""
    user_message = request.message
    
    # Save user message
    user_chat = ChatHistory(
        user_id=user_id,
        role="user",
        message=user_message
    )
    db.add(user_chat)
    db.commit()
    
    # Search for relevant knowledge nodes using RAG
    related_nodes = []
    context = ""
    
    if request.use_rag:
        try:
            relevant = rag_service.search_relevant_nodes(user_message, top_k=3)
            related_nodes = [node['node_id'] for node in relevant]
            context = rag_service.build_context_from_nodes(relevant)
        except Exception as e:
            print(f"RAG search error: {e}")
    
    # Build messages for AI
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ Frontend Master çš„ AI åŠ©æ•™,ä¸“é—¨å¸®åŠ©å­¦ä¹ è€…ç†è§£å‰ç«¯æŠ€æœ¯ã€‚è¯·ç”¨ç®€æ´ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚å¦‚æœæä¾›äº†ç›¸å…³çŸ¥è¯†ç‚¹,è¯·ç»“åˆè¿™äº›å†…å®¹å›ç­”ã€‚"}
    ]
    
    if context:
        messages.append({"role": "system", "content": context})
    
    # Get recent chat history
    try:
        recent_chats = db.query(ChatHistory).filter(
            ChatHistory.user_id == user_id
        ).order_by(ChatHistory.created_at.desc()).limit(6).all()
        
        for chat in reversed(recent_chats):
            messages.append({
                "role": "user" if chat.role == "user" else "assistant",
                "content": chat.message
            })
    except:
        pass
    
    messages.append({"role": "user", "content": user_message})
    
    # Get AI response
    ai_response = await ai_service.chat_completion(messages)
    
    # Save AI response
    ai_chat = ChatHistory(
        user_id=user_id,
        role="assistant",
        message=ai_response,
        related_nodes=related_nodes
    )
    db.add(ai_chat)
    db.commit()
    
    return SuccessResponse(
        data={
            "message": ai_response,
            "related_nodes": related_nodes
        }
    )


@router.get("/history")
def get_chat_history(
    user_id: int = 1,
    limit: int = 50,
    db: Session = Depends(get_db)
):
    """Get chat history for a user"""
    try:
        chats = db.query(ChatHistory).filter(
            ChatHistory.user_id == user_id
        ).order_by(ChatHistory.created_at.desc()).limit(limit).all()
        
        chat_list = []
        for chat in reversed(chats):
            chat_list.append({
                "id": chat.id,
                "role": chat.role,
                "message": chat.message,
                "related_nodes": chat.related_nodes or [],
                "created_at": chat.created_at
            })
        
        return SuccessResponse(data=chat_list)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/history")
def clear_chat_history(
    user_id: int = 1,
    db: Session = Depends(get_db)
):
    """Clear chat history for a user"""
    try:
        db.query(ChatHistory).filter(ChatHistory.user_id == user_id).delete()
        db.commit()
        
        return SuccessResponse(data={"message": "Chat history cleared"})
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/code-assist")
async def code_assist_endpoint(request: CodeAssistRequest):
    """
    ä»£ç è¾…åŠ©æ¥å£ - è§£é‡Šã€è¡¥å…¨ã€ä¿®æ”¹ä»£ç 
    """
    code = request.code
    action = request.action
    language = request.language
    context = request.context
    
    print(f"\n{'='*60}")
    print(f"ğŸ”§ ä»£ç è¾…åŠ©è¯·æ±‚")
    print(f"æ“ä½œ: {action}")
    print(f"è¯­è¨€: {language}")
    print(f"ä»£ç é•¿åº¦: {len(code)} å­—ç¬¦")
    print(f"{'='*60}\n")
    
    # æ ¹æ®ä¸åŒçš„æ“ä½œç”Ÿæˆä¸åŒçš„æç¤ºè¯
    prompts = {
        "explain": f"""è¯·åˆ†æè¿™æ®µ {language} ä»£ç ï¼š

{code}

{f"ç¯å¢ƒï¼š{context}" if context else ""}

è¯´æ˜ä»£ç åŠŸèƒ½ã€å…³é”®è¯­æ³•å’Œæ”¹è¿›å»ºè®®ã€‚""",

        "complete": f"""è¯·è¡¥å…¨è¿™æ®µ {language} ä»£ç ï¼š

{code}

{f"ç¯å¢ƒï¼š{context}" if context else ""}

æä¾›å®Œæ•´ä»£ç å’Œè¯´æ˜ã€‚""",

        "fix": f"""è¯·ä¿®å¤è¿™æ®µ {language} ä»£ç ï¼š

{code}

{f"ç¯å¢ƒï¼š{context}" if context else ""}

ç›´æ¥ç»™å‡ºä¿®å¤åçš„ä»£ç å’Œä¿®æ”¹åŸå› ã€‚"""
    }
    
    prompt = prompts.get(action, prompts["explain"])
    
    async def generate():
        try:
            async for chunk in ai_service.chat_stream_with_context(prompt):
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            error_msg = f"ä»£ç è¾…åŠ©æœåŠ¡é”™è¯¯: {str(e)}"
            print(f"\nâŒ {error_msg}\n")
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
