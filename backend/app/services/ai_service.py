from typing import AsyncGenerator, List, Dict, Optional
from ..core.config import settings

# åŠ¨æ€å¯¼å…¥ OpenAI ä»¥é¿å…åˆå§‹åŒ–é—®é¢˜
client = None

def get_openai_client():
    global client
    if client is None:
        print(f"\n{'='*60}")
        print(f"ğŸ” æ£€æŸ¥ API é…ç½®:")
        print(f"   - OPENAI_API_KEY å­˜åœ¨: {bool(settings.OPENAI_API_KEY)}")
        print(f"   - API Key é•¿åº¦: {len(settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else 0}")
        print(f"   - API Base: {settings.OPENAI_API_BASE}")
        print(f"   - Model: {settings.OPENAI_MODEL}")
        
        if settings.OPENAI_API_KEY:
            print(f"   - API Key å€¼: {settings.OPENAI_API_KEY}")
            print(f"{'='*60}\n")
        else:
            print(f"   âš ï¸ API Key æœªè®¾ç½®ï¼")
            print(f"{'='*60}\n")
            return None
            
        try:
            from openai import OpenAI
            # æ”¯æŒè‡ªå®šä¹‰ API Base URL
            if settings.OPENAI_API_BASE:
                # OpenAIå®¢æˆ·ç«¯ä¼šè‡ªåŠ¨æ·»åŠ  "Authorization: Bearer {api_key}"
                client = OpenAI(
                    api_key=settings.OPENAI_API_KEY,
                    base_url=settings.OPENAI_API_BASE
                )
                print(f"âœ… OpenAI Client åˆå§‹åŒ–æˆåŠŸ")
                print(f"ğŸ“¡ å®é™…ä½¿ç”¨çš„é…ç½®:")
                print(f"   - Base URL: {client.base_url}")
                print(f"   - API Key: {client.api_key[:15]}...{client.api_key[-5:]}")
            else:
                client = OpenAI(api_key=settings.OPENAI_API_KEY)
                print(f"âœ… OpenAI Client åˆå§‹åŒ–æˆåŠŸ (å®˜æ–¹ API)")
        except ImportError:
            print("âŒ OpenAI library not available")
        except Exception as e:
            print(f"âŒ Failed to initialize OpenAI client: {e}")
    return client

SYSTEM_PROMPT = """ä½ æ˜¯ Frontend Master çš„ AI åŠ©æ•™ï¼Œä¸“é—¨å¸®åŠ©å­¦ä¹ è€…ç†è§£å‰ç«¯æŠ€æœ¯ã€‚
è¯·ç”¨ç®€æ´ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚å¦‚æœæä¾›äº†ç›¸å…³çŸ¥è¯†ç‚¹ï¼Œè¯·ç»“åˆè¿™äº›å†…å®¹å›ç­”ã€‚
ä½¿ç”¨ Markdown æ ¼å¼ï¼Œå¹¶å§‹ç»ˆä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›ç­”ã€‚

ä½ æ“…é•¿çš„é¢†åŸŸåŒ…æ‹¬:
- HTML5, CSS3, JavaScript, TypeScript
- Vue.js, React, Angular ç­‰å‰ç«¯æ¡†æ¶
- Webpack, Vite, Rollup ç­‰æ„å»ºå·¥å…·
- TailwindCSS, Sass ç­‰æ ·å¼æ–¹æ¡ˆ
- å‰ç«¯æ€§èƒ½ä¼˜åŒ–å’Œæ¶æ„è®¾è®¡
- Web å®‰å…¨å’Œæœ€ä½³å®è·µ

è¯·æä¾›æ¸…æ™°ã€å®ç”¨çš„å»ºè®®å’Œä»£ç ç¤ºä¾‹ã€‚"""


class AIService:
    def __init__(self):
        self.client = None
    
    def get_client(self):
        if self.client is None:
            self.client = get_openai_client()
        return self.client
    
    async def chat_stream_with_context(
        self,
        message: str,
        context: str = "",
        history: Optional[List[Dict]] = None
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦ä¸Šä¸‹æ–‡çš„æµå¼ AI å›å¤
        """
        client = self.get_client()
        if not client:
            print("âŒ AI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            yield "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
            return
        
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # æ·»åŠ  RAG ä¸Šä¸‹æ–‡
        if context:
            messages.append({"role": "system", "content": context})
            print(f"ğŸ“š æ·»åŠ  RAG ä¸Šä¸‹æ–‡: {len(context)} å­—ç¬¦")
        
        # æ·»åŠ å†å²å¯¹è¯
        if history:
            messages.extend(history)
            print(f"ğŸ’¬ æ·»åŠ å†å²å¯¹è¯: {len(history)} æ¡")
        
        messages.append({"role": "user", "content": message})
        
        print("\n" + "="*60)
        print(f"ğŸ¤” ç”¨æˆ·æé—®: {message}")
        print(f"ğŸ“ æ€»æ¶ˆæ¯æ•°: {len(messages)}")
        print("="*60)
        
        try:
            print(f"ğŸš€ å¼€å§‹è°ƒç”¨ {settings.OPENAI_MODEL} æ¨¡å‹...")
            print(f"ğŸ“¤ è¯·æ±‚å‚æ•°:")
            print(f"   - Model: {settings.OPENAI_MODEL}")
            print(f"   - Messages: {len(messages)} æ¡")
            print(f"   - Stream: True")
            print(f"   - Temperature: 0.7")
            print(f"   - Max Tokens: 2000\n")
            
            stream = client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                stream=True,
                temperature=0.7,
                max_tokens=10000
            )
            
            print("ğŸ’­ AI æ€è€ƒè¿‡ç¨‹:")
            print("-" * 60)
            
            full_response = ""
            chunk_count = 0
            for chunk in stream:
                chunk_count += 1
                
                if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                    choice = chunk.choices[0]
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«å†…å®¹è¿‡æ»¤å™¨æ‹¦æˆª
                    if hasattr(choice, 'finish_reason') and choice.finish_reason == 'content_filter':
                        error_msg = "âš ï¸ å†…å®¹è¢« API å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆªï¼Œè¯·å°è¯•ç®€åŒ–æç¤ºè¯"
                        print(f"\n{error_msg}\n")
                        yield error_msg
                        break
                    
                    if hasattr(choice, 'delta'):
                        delta = choice.delta
                        
                        if hasattr(delta, 'content') and delta.content:
                            content = delta.content
                            full_response += content
                            print(content, end="", flush=True)
                            yield content
            
            print("\n" + "-" * 60)
            print(f"âœ… å›ç­”å®Œæˆ! æ€»å­—ç¬¦æ•°: {len(full_response)}, æ€»chunks: {chunk_count}")
            print("="*60 + "\n")
            
            if len(full_response) == 0:
                error_msg = "âš ï¸ AI è¿”å›äº†ç©ºå“åº”ï¼Œè¯·æ£€æŸ¥ API é…ç½®æˆ–é‡è¯•"
                print(f"{error_msg}\n")
                yield error_msg
                    
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {str(e)}"
            print(f"\nâŒ é”™è¯¯: {error_msg}")
            print("="*60 + "\n")
            yield error_msg
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]]
    ) -> str:
        """
        å®Œæ•´è¿”å› AI å›å¤ (éæµå¼)
        """
        client = self.get_client()
        if not client:
            return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
        
        try:
            response = client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=10000
            )
            
            return response.choices[0].message.content or "æˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            
        except Exception as e:
            return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {str(e)}"


# Legacy functions for backward compatibility
async def chat_stream(message: str, history: list = None) -> AsyncGenerator[str, None]:
    """
    æµå¼è¿”å› AI å›å¤ (å‘åå…¼å®¹)
    """
    ai_service = AIService()
    async for chunk in ai_service.chat_stream_with_context(message, "", history):
        yield chunk


async def chat_complete(message: str, history: list = None) -> str:
    """
    å®Œæ•´è¿”å› AI å›å¤ (éæµå¼)
    """
    openai_client = get_openai_client()
    if not openai_client:
        return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
    
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    if history:
        messages.extend(history)
    
    messages.append({"role": "user", "content": message})
    
    try:
        response = openai_client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=10000
        )
        
        return response.choices[0].message.content or "æˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        
    except Exception as e:
        return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {str(e)}"


# Singleton instance
ai_service = AIService()
