# from functools import lru_cache
# from typing import AsyncGenerator, List, Dict, Optional
# from ..core.config import settings

# try:  # Optional OpenAI import to keep backend boot resilient during local setup
#     from openai import OpenAI, OpenAIError  # type: ignore
# except ImportError:  # pragma: no cover - handled gracefully at runtime
#     OpenAI = None  # type: ignore

#     class OpenAIError(RuntimeError):  # type: ignore
#         """Fallback error class when OpenAI SDK is unavailable."""

# @lru_cache(maxsize=1)
# def get_openai_client():
#     if OpenAI is None:
#         print("OpenAI library not available")
#         return None

#     if not settings.OPENAI_API_KEY:
#         return None

#     try:
#         return OpenAI(
#             api_key=settings.OPENAI_API_KEY,
#             base_url=settings.OPENAI_API_BASE or None
#         )
#     except (OpenAIError, ValueError, RuntimeError) as exc:
#         print(f"Failed to initialize OpenAI client: {exc}")
#         return None

# SYSTEM_PROMPT = """
# ä½ æ˜¯ä¸€ä½äº²åˆ‡ã€ä¸“ä¸šä¸”å¯Œæœ‰è€å¿ƒçš„"å‰ç«¯ AI åŠ©æ•™"ã€‚
# ä½ çš„ç›®æ ‡æ˜¯åƒå­¦é•¿æˆ–å¯¼å¸ˆä¸€æ ·ï¼Œé™ªä¼´å­¦ç”Ÿæ”»å…‹å‰ç«¯å­¦ä¹ ä¸­çš„éš¾å…³ã€‚

# ã€äººè®¾ä¸è¯­æ°”ã€‘
# 1. **äº²åˆ‡å‹å¥½**ï¼šè¯­æ°”è¦æ¸©æš–ã€é¼“åŠ±ï¼Œå¤šä½¿ç”¨"æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹"ã€"åˆ«æ‹…å¿ƒ"ã€"åŠ æ²¹"ç­‰è¯æ±‡ã€‚
# 2. **é€šä¿—æ˜“æ‡‚**ï¼šåœ¨æ·±å…¥åº•å±‚åŸç†å‰ï¼Œå…ˆç”¨ç”ŸåŠ¨å½¢è±¡çš„æ¯”å–»æˆ–ç”Ÿæ´»åŒ–çš„ä¾‹å­æ¥è§£é‡Šå¤æ‚æ¦‚å¿µã€‚
# 3. **äº’åŠ¨å¼•å¯¼**ï¼šåœ¨å›ç­”ç»“æŸæ—¶ï¼Œå¯ä»¥é€‚å½“åœ°ç»™å­¦ç”Ÿä¸€ä¸ªå°å»ºè®®æˆ–ä¸€ä¸ªå°æŒ‘æˆ˜ï¼Œå¼•å¯¼ä»–ä»¬åŠ¨æ‰‹å®è·µã€‚
# 4. **è¡¨æƒ…ç‚¹ç¼€**ï¼šåœ¨æ­£æ–‡ä¸­è‡ªç„¶åœ°ä½¿ç”¨ Emojiï¼ˆå¦‚ âœ¨, ğŸ’¡, ğŸš€, âœ…, ğŸŒˆï¼‰ï¼Œè®©å›ç­”çœ‹èµ·æ¥æ›´æœ‰æ´»åŠ›ï¼Œä¸æ¯ç‡¥ã€‚

# ã€æ ¸å¿ƒç›®æ ‡ã€‘
# 1. **å†…å®¹æ·±åº¦**ï¼šå›ç­”è¦è¯¦å°½ä¸”å…·æœ‰æ·±åº¦ã€‚ä½œä¸ºåŠ©æ•™ï¼Œä½ ä¸ä»…è¦ç»™å‡ºç»“è®ºï¼Œæ›´è¦æ·±å…¥æµ…å‡ºåœ°è®²è§£åº•å±‚åŸç†ã€æœ€ä½³å®è·µä»¥åŠæ½œåœ¨çš„å‘ã€‚
# 2. **æè‡´æ¢è¡Œ**ï¼šæ¯ä¸€æ®µè¯ã€æ¯ä¸€ä¸ªåˆ—è¡¨é¡¹ã€æ¯ä¸€ä¸ªä»£ç å—ä¹‹é—´å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç©ºè¡Œã€‚ä¸¥ç¦æ–‡å­—å †ç Œï¼Œç¡®ä¿é¡µé¢æœ‰è¶³å¤Ÿçš„"å‘¼å¸æ„Ÿ"ã€‚
# 3. **è§†è§‰å¼•å¯¼**ï¼šé€šè¿‡æ ‡é¢˜ï¼ˆ##ï¼‰ã€åŠ ç²—ï¼ˆ**å…³é”®è¯**ï¼‰å’Œä»£ç å—å¼•å¯¼ç”¨æˆ·é˜…è¯»ã€‚

# ã€æ’ç‰ˆä¸æ ¼å¼è§„èŒƒã€‘
# 1. **ä¸¥ç¦æ ‡é¢˜å†—é•¿**ï¼šäºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰å¿…é¡»çŸ­å°ç²¾æ‚ï¼ˆä¸è¶…è¿‡ 15 ä¸ªå­—ï¼‰ã€‚ä¸¥ç¦å°†é•¿å¥å­ä½œä¸ºæ ‡é¢˜ã€‚
# 2. **ä¸¥ç¦æ ‡é¢˜æ‹†åˆ†**ï¼šæ ‡é¢˜å¿…é¡»åœ¨ä¸€è¡Œå†…å†™å®Œï¼Œä¸¥ç¦å°†æ ‡é¢˜å†…å®¹æ¢è¡Œå†™ã€‚
# 3. **ä¸¥ç¦æ ‡é¢˜ç²˜è¿**ï¼šæ ‡é¢˜è¡Œä¹‹åå¿…é¡»ç´§è·Ÿä¸€ä¸ªç©ºè¡Œï¼Œä¸¥ç¦ç›´æ¥å¼€å§‹æ­£æ–‡ã€‚
# 4. **ä¸¥ç¦éæ ‡åˆ—è¡¨**ï¼šå¿…é¡»ä½¿ç”¨æ ‡å‡†çš„ Markdown åˆ—è¡¨ `- ` æˆ– `1. `ã€‚ç¬¦å·åå¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç©ºæ ¼ã€‚
# 5. **ä¸¥ç¦æ–‡å­—å †ç Œ**ï¼šç¦æ­¢è¾“å‡ºè¶…è¿‡ 4 è¡Œçš„çº¯æ–‡å­—æ®µè½ï¼Œé•¿æ®µè½å¿…é¡»æ‹†åˆ†ã€‚
# 6. **ä¸¥ç¦ç¬¦å·æ±¡æŸ“**ï¼šä¸¥ç¦åœ¨æ ‡é¢˜ï¼ˆ## æˆ– ###ï¼‰ä¸­ä½¿ç”¨ `**` åŠ ç²—ç¬¦å·ã€‚ä¸¥ç¦ä½¿ç”¨ `**æ ‡é¢˜**` è¿™ç§å½¢å¼æ¥ä»£æ›¿ Markdown æ ‡é¢˜ã€‚
# 7. **ä¸¥ç¦ç©ºä»£ç å—**ï¼šå¦‚æœæ²¡æœ‰ä»£ç ï¼Œä¸¥ç¦è¾“å‡ºç©ºçš„ ` ``` ` å—ã€‚
# 8. **ä¸¥ç¦åŠ ç²—æ»¥ç”¨**ï¼šä¸¥ç¦å¯¹æ•´æ®µæ–‡å­—åŠ ç²—ï¼Œä»…å¯¹æ ¸å¿ƒæœ¯è¯­åŠ ç²—ã€‚åŠ ç²—ç¬¦å· `**` å‰åå¿…é¡»æœ‰ç©ºæ ¼ï¼ˆé™¤éåœ¨è¡Œé¦–æˆ–æ ‡ç‚¹å‰ï¼‰ã€‚
# 9. **ä¸¥ç¦é”™è¯¯åŒ…è£¹**ï¼šä¸¥ç¦å°†æç¤ºã€å»ºè®®ã€å¼•ç”¨ï¼ˆä»¥ > å¼€å¤´çš„å†…å®¹ï¼‰åŒ…è£¹åœ¨ä»£ç å—ï¼ˆ```ï¼‰ä¸­ã€‚

# ã€å¿…é¡»éµå®ˆçš„ç»“æ„ã€‘
# 1. **äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰**ï¼šæ ¼å¼å¿…é¡»ä¸º `## ğŸš€ æ ‡é¢˜å†…å®¹`ã€‚
# 2. **æè‡´æ¢è¡Œè§„èŒƒ**ï¼š
#    - æ ‡é¢˜ä¸æ­£æ–‡ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
#    - æ®µè½ä¸æ®µè½ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
#    - åˆ—è¡¨é¡¹ä¸åˆ—è¡¨é¡¹ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
#    - ä»£ç å—å‰åï¼šå¿…é¡»å„ç©ºä¸€è¡Œã€‚

# 3. **ä»£ç å—è§„èŒƒ**ï¼š
#    - å¿…é¡»æŒ‡æ˜ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ ` ```javascript `ï¼‰ã€‚
#    - ä»£ç ç¼©è¿›å¿…é¡»ç»Ÿä¸€ï¼ˆä½¿ç”¨ 2 ä¸ªç©ºæ ¼ï¼‰ã€‚
#    - ä¸¥ç¦åœ¨ä»£ç å—ä¸­æ··å…¥è§£é‡Šæ€§æ–‡å­—ã€‚
# """


# class AIService:
#     def __init__(self):
#         self.client = None
    
#     def get_client(self):
#         if self.client is None:
#             self.client = get_openai_client()
#         return self.client
    
#     async def chat_stream_with_context(
#         self,
#         message: str,
#         context: str = "",
#         history: Optional[List[Dict]] = None
#     ) -> AsyncGenerator[str, None]:
#         """
#         å¸¦ä¸Šä¸Šä¸‹æ–‡çš„æµå¼ AI å›å¤
#         """
#         openai_client = self.get_client()
#         if not openai_client:
#             yield "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
#             return
        
#         system_content = SYSTEM_PROMPT
#         if context:
#             system_content += f"\n\nã€å‚è€ƒçŸ¥è¯†åº“å†…å®¹ã€‘\n{context}\n\nè¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° Markdown æ ¼å¼è§„èŒƒè¿›è¡Œå›å¤ã€‚"
            
#         messages = [{"role": "system", "content": system_content}]
        
#         if history:
#             messages.extend(history)
        
#         messages.append({"role": "user", "content": message})
        
#         try:
#             response = openai_client.chat.completions.create(
#                 model=settings.OPENAI_MODEL,
#                 messages=messages,
#                 stream=True,
#                 temperature=0.7,
#                 max_tokens=2000
#             )
            
#             for chunk in response:
#                 if chunk.choices and chunk.choices[0].delta.content:
#                     yield chunk.choices[0].delta.content
#         except (OpenAIError, ValueError, RuntimeError) as exc:
#             yield f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {exc}"

#     async def chat_with_context(
#         self,
#         message: str,
#         context: str = "",
#         history: Optional[List[Dict]] = None
#     ) -> str:
#         """
#         å¸¦ä¸Šä¸Šä¸‹æ–‡çš„éæµå¼ AI å›å¤
#         """
#         client = self.get_client()
#         if not client:
#             return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
        
#         system_content = SYSTEM_PROMPT
#         if context:
#             system_content += f"\n\nã€å‚è€ƒçŸ¥è¯†åº“å†…å®¹ã€‘\n{context}\n\nè¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° Markdown æ ¼å¼è§„èŒƒè¿›è¡Œå›å¤ã€‚"
            
#         messages = [{"role": "system", "content": system_content}]
        
#         if history:
#             messages.extend(history)
        
#         messages.append({"role": "user", "content": message})
        
#         try:
#             response = client.chat.completions.create(
#                 model=settings.OPENAI_MODEL,
#                 messages=messages,
#                 temperature=0.7,
#                 max_tokens=2000
#             )
            
#             choices = getattr(response, "choices", []) or []
#             if not choices:
#                 return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªè¿”å›æœ‰æ•ˆç»“æœã€‚"

#             message = getattr(choices[0], "message", None)
#             content = getattr(message, "content", None)
#             return content or "æˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
#         except (OpenAIError, ValueError, RuntimeError) as exc:
#             return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {exc}"


# # Singleton instance
# ai_service = AIService()
from functools import lru_cache
from typing import AsyncGenerator, List, Dict, Optional
from ..core.config import settings

try:  # Optional OpenAI import to keep backend boot resilient during local setup
    from openai import OpenAI, OpenAIError  # type: ignore
except ImportError:  # pragma: no cover - handled gracefully at runtime
    OpenAI = None  # type: ignore

    class OpenAIError(RuntimeError):  # type: ignore
        """Fallback error class when OpenAI SDK is unavailable."""

@lru_cache(maxsize=1)
def get_openai_client():
    if OpenAI is None:
        print("OpenAI library not available")
        return None

    if not settings.OPENAI_API_KEY:
        return None

    try:
        return OpenAI(
            api_key=settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_API_BASE or None
        )
    except (OpenAIError, ValueError, RuntimeError) as exc:
        print(f"Failed to initialize OpenAI client: {exc}")
        return None

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½äº²åˆ‡ã€ä¸“ä¸šä¸”å¯Œæœ‰è€å¿ƒçš„â€œå‰ç«¯ AI åŠ©æ•™â€ã€‚
ä½ çš„ç›®æ ‡æ˜¯åƒå­¦é•¿æˆ–å¯¼å¸ˆä¸€æ ·ï¼Œé™ªä¼´å­¦ç”Ÿæ”»å…‹å‰ç«¯å­¦ä¹ ä¸­çš„éš¾å…³ã€‚

ã€äººè®¾ä¸è¯­æ°”ã€‘
1. **äº²åˆ‡å‹å¥½**ï¼šè¯­æ°”è¦æ¸©æš–ã€é¼“åŠ±ï¼Œå¤šä½¿ç”¨â€œæˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹â€ã€â€œåˆ«æ‹…å¿ƒâ€ã€â€œåŠ æ²¹â€ç­‰è¯æ±‡ã€‚
2. **é€šä¿—æ˜“æ‡‚**ï¼šåœ¨æ·±å…¥åº•å±‚åŸç†å‰ï¼Œå…ˆç”¨ç”ŸåŠ¨å½¢è±¡çš„æ¯”å–»æˆ–ç”Ÿæ´»åŒ–çš„ä¾‹å­æ¥è§£é‡Šå¤æ‚æ¦‚å¿µã€‚
3. **äº’åŠ¨å¼•å¯¼**ï¼šåœ¨å›ç­”ç»“æŸæ—¶ï¼Œå¯ä»¥é€‚å½“åœ°ç»™å­¦ç”Ÿä¸€ä¸ªå°å»ºè®®æˆ–ä¸€ä¸ªå°æŒ‘æˆ˜ï¼Œå¼•å¯¼ä»–ä»¬åŠ¨æ‰‹å®è·µã€‚
4. **è¡¨æƒ…ç‚¹ç¼€**ï¼šåœ¨æ­£æ–‡ä¸­è‡ªç„¶åœ°ä½¿ç”¨ Emojiï¼ˆå¦‚ âœ¨, ğŸ’¡, ğŸš€, âœ…, ğŸŒˆï¼‰ï¼Œè®©å›ç­”çœ‹èµ·æ¥æ›´æœ‰æ´»åŠ›ï¼Œä¸æ¯ç‡¥ã€‚

ã€æ ¸å¿ƒç›®æ ‡ã€‘
1. **å†…å®¹æ·±åº¦**ï¼šå›ç­”è¦è¯¦å°½ä¸”å…·æœ‰æ·±åº¦ã€‚ä½œä¸ºåŠ©æ•™ï¼Œä½ ä¸ä»…è¦ç»™å‡ºç»“è®ºï¼Œæ›´è¦æ·±å…¥æµ…å‡ºåœ°è®²è§£åº•å±‚åŸç†ã€æœ€ä½³å®è·µä»¥åŠæ½œåœ¨çš„å‘ã€‚
2. **æè‡´æ¢è¡Œ**ï¼šæ¯ä¸€æ®µè¯ã€æ¯ä¸€ä¸ªåˆ—è¡¨é¡¹ã€æ¯ä¸€ä¸ªä»£ç å—ä¹‹é—´å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç©ºè¡Œã€‚ä¸¥ç¦æ–‡å­—å †ç Œï¼Œç¡®ä¿é¡µé¢æœ‰è¶³å¤Ÿçš„â€œå‘¼å¸æ„Ÿâ€ã€‚
3. **è§†è§‰å¼•å¯¼**ï¼šé€šè¿‡æ ‡é¢˜ï¼ˆ##ï¼‰ã€åŠ ç²—ï¼ˆ**å…³é”®è¯**ï¼‰å’Œä»£ç å—å¼•å¯¼ç”¨æˆ·é˜…è¯»ã€‚

ã€æ’ç‰ˆä¸æ ¼å¼è§„èŒƒã€‘
1. **ä¸¥ç¦æ ‡é¢˜å†—é•¿**ï¼šäºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰å¿…é¡»çŸ­å°ç²¾æ‚ï¼ˆä¸è¶…è¿‡ 15 ä¸ªå­—ï¼‰ã€‚ä¸¥ç¦å°†é•¿å¥å­ä½œä¸ºæ ‡é¢˜ã€‚
2. **ä¸¥ç¦æ ‡é¢˜æ‹†åˆ†**ï¼šæ ‡é¢˜å¿…é¡»åœ¨ä¸€è¡Œå†…å†™å®Œï¼Œä¸¥ç¦å°†æ ‡é¢˜å†…å®¹æ¢è¡Œå†™ã€‚
3. **ä¸¥ç¦æ ‡é¢˜ç²˜è¿**ï¼šæ ‡é¢˜è¡Œä¹‹åå¿…é¡»ç´§è·Ÿä¸€ä¸ªç©ºè¡Œï¼Œä¸¥ç¦ç›´æ¥å¼€å§‹æ­£æ–‡ã€‚
4. **ä¸¥ç¦éæ ‡åˆ—è¡¨**ï¼šå¿…é¡»ä½¿ç”¨æ ‡å‡†çš„ Markdown åˆ—è¡¨ `- ` æˆ– `1. `ã€‚ç¬¦å·åå¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªç©ºæ ¼ã€‚
5. **ä¸¥ç¦æ–‡å­—å †ç Œ**ï¼šç¦æ­¢è¾“å‡ºè¶…è¿‡ 4 è¡Œçš„çº¯æ–‡å­—æ®µè½ï¼Œé•¿æ®µè½å¿…é¡»æ‹†åˆ†ã€‚
6. **ä¸¥ç¦ç¬¦å·æ±¡æŸ“**ï¼šä¸¥ç¦åœ¨æ ‡é¢˜ï¼ˆ## æˆ– ###ï¼‰ä¸­ä½¿ç”¨ `**` åŠ ç²—ç¬¦å·ã€‚ä¸¥ç¦ä½¿ç”¨ `**æ ‡é¢˜**` è¿™ç§å½¢å¼æ¥ä»£æ›¿ Markdown æ ‡é¢˜ã€‚
7. **ä¸¥ç¦ç©ºä»£ç å—**ï¼šå¦‚æœæ²¡æœ‰ä»£ç ï¼Œä¸¥ç¦è¾“å‡ºç©ºçš„ ` ``` ` å—ã€‚
8. **ä¸¥ç¦åŠ ç²—æ»¥ç”¨**ï¼šä¸¥ç¦å¯¹æ•´æ®µæ–‡å­—åŠ ç²—ï¼Œä»…å¯¹æ ¸å¿ƒæœ¯è¯­åŠ ç²—ã€‚åŠ ç²—ç¬¦å· `**` å‰åå¿…é¡»æœ‰ç©ºæ ¼ï¼ˆé™¤éåœ¨è¡Œé¦–æˆ–æ ‡ç‚¹å‰ï¼‰ã€‚
9. **ä¸¥ç¦é”™è¯¯åŒ…è£¹**ï¼šä¸¥ç¦å°†æç¤ºã€å»ºè®®ã€å¼•ç”¨ï¼ˆä»¥ > å¼€å¤´çš„å†…å®¹ï¼‰åŒ…è£¹åœ¨ä»£ç å—ï¼ˆ```ï¼‰ä¸­ã€‚

ã€å¿…é¡»éµå®ˆçš„ç»“æ„ã€‘
1. **äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰**ï¼šæ ¼å¼å¿…é¡»ä¸º `## ğŸš€ æ ‡é¢˜å†…å®¹`ã€‚
2. **æè‡´æ¢è¡Œè§„èŒƒ**ï¼š
   - æ ‡é¢˜ä¸æ­£æ–‡ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
   - æ®µè½ä¸æ®µè½ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
   - åˆ—è¡¨é¡¹ä¸åˆ—è¡¨é¡¹ä¹‹é—´ï¼šå¿…é¡»ç©ºä¸€è¡Œã€‚
   - ä»£ç å—å‰åï¼šå¿…é¡»å„ç©ºä¸€è¡Œã€‚

3. **ä»£ç å—è§„èŒƒ**ï¼š
   - å¿…é¡»æŒ‡æ˜ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ ` ```javascript `ï¼‰ã€‚
   - ä»£ç ç¼©è¿›å¿…é¡»ç»Ÿä¸€ï¼ˆä½¿ç”¨ 2 ä¸ªç©ºæ ¼ï¼‰ã€‚
   - ä¸¥ç¦åœ¨ä»£ç å—ä¸­æ··å…¥è§£é‡Šæ€§æ–‡å­—ã€‚
"""


class AIService:
    def __init__(self):
        self.client = None
    
    def get_client(self):
        if self.client is None:
            self.client = get_openai_client()
        return self.client
    
    async def chat_stream_with_context(
        self,
        message: str,
        context: str = "",
        history: Optional[List[Dict]] = None
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦ä¸Šä¸Šä¸‹æ–‡çš„æµå¼ AI å›å¤
        """
        openai_client = self.get_client()
        if not openai_client:
            yield "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
            return
        
        system_content = SYSTEM_PROMPT
        if context:
            system_content += f"\n\nã€å‚è€ƒçŸ¥è¯†åº“å†…å®¹ã€‘\n{context}\n\nè¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° Markdown æ ¼å¼è§„èŒƒè¿›è¡Œå›å¤ã€‚"
            
        messages = [{"role": "system", "content": system_content}]
        
        if history:
            messages.extend(history)
        
        messages.append({"role": "user", "content": message})
        
        try:
            response = openai_client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                stream=True,
                temperature=0.7,
                max_tokens=2000
            )
            
            for chunk in response:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except (OpenAIError, ValueError, RuntimeError) as exc:
            yield f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {exc}"

    async def chat_with_context(
        self,
        message: str,
        context: str = "",
        history: Optional[List[Dict]] = None
    ) -> str:
        """
        å¸¦ä¸Šä¸Šä¸‹æ–‡çš„éæµå¼ AI å›å¤
        """
        client = self.get_client()
        if not client:
            return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªé…ç½® API Keyï¼Œè¯·è”ç³»ç®¡ç†å‘˜ã€‚"
        
        system_content = SYSTEM_PROMPT
        if context:
            system_content += f"\n\nã€å‚è€ƒçŸ¥è¯†åº“å†…å®¹ã€‘\n{context}\n\nè¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿° Markdown æ ¼å¼è§„èŒƒè¿›è¡Œå›å¤ã€‚"
            
        messages = [{"role": "system", "content": system_content}]
        
        if history:
            messages.extend(history)
        
        messages.append({"role": "user", "content": message})
        
        try:
            response = client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            choices = getattr(response, "choices", []) or []
            if not choices:
                return "æŠ±æ­‰ï¼ŒAI æœåŠ¡æœªè¿”å›æœ‰æ•ˆç»“æœã€‚"

            message = getattr(choices[0], "message", None)
            content = getattr(message, "content", None)
            return content or "æˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        except (OpenAIError, ValueError, RuntimeError) as exc:
            return f"æŠ±æ­‰ï¼ŒAI æœåŠ¡å‡ºç°é”™è¯¯: {exc}"


# Singleton instance
ai_service = AIService()